{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanderbilt-data-science/ai-summer-2025/blob/main/PEFT_LoRA_llama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning LLaMa 3.2\n",
        "\n",
        "This notebook demonstrates how to fine-tune the LLaMa 3.2 (1B model), Meta's latest small-scale LLM. We use the 1B parameter version to ensure this can run effectively on Google Colab.\n",
        "\n",
        "Key features of LLaMa 3.2 1B:\n",
        "- 1.23B parameters\n",
        "- Multilingual support (8 officially supported languages)\n",
        "- 128k context length\n",
        "- Optimized for dialogue use cases\n",
        "\n",
        "As we'll see, this same notebook can be used to train other, larger versions of LlaMa 3.2, simply by swapping the HuggingFace repo to one of the larger versions.\n",
        "\n"
      ],
      "metadata": {
        "id": "gAsvqrahL8WM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Setup\n"
      ],
      "metadata": {
        "id": "z-ft6hpQK_PU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Package Installation + Imports\n",
        "\n",
        "First, let's install the required packages. We'll need the latest version of transformers (>= 4.43.0) to work with LLaMa 3.2."
      ],
      "metadata": {
        "id": "hDYxyiRTLIRu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYjfbQJZH29C"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q --upgrade transformers datasets\n",
        "!pip install -q torch accelerate bitsandbytes\n",
        "# Install PDF processing libraries. pdfplumber handles academic texts better\n",
        "!pip install -q PyPDF2\n",
        "!pip install -q pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard libraries\n",
        "import re\n",
        "import os\n",
        "import numpy as np\n",
        "from typing import List\n",
        "\n",
        "# AI/ML Libraries\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import pipeline\n",
        "from accelerate.state import AcceleratorState\n",
        "from accelerate import Accelerator\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import wandb # this will be optional, for training monitoring\n",
        "\n",
        "# PDF reading\n",
        "import pdfplumber\n",
        "from pdfminer.high_level import extract_text\n",
        "from PyPDF2 import PdfReader"
      ],
      "metadata": {
        "id": "q_yvuVAUxucp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Package Overview\n",
        "- `transformers`: Hugging Face's main library for working with transformer models\n",
        "- `torch`: PyTorch deep learning framework\n",
        "- `accelerate`: Library for easy mixed precision training and device placement\n",
        "- `pdfminer`: A PDF-reading package that's particularly good for non-straightforward PDF's (like academic papers)\n"
      ],
      "metadata": {
        "id": "HiOJjfe7RLlH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Check\n",
        "Let's verify our setup and check available compute resources:\n",
        "\n",
        "Make sure that `CUDA available`: prints **True**"
      ],
      "metadata": {
        "id": "3mjl7aNgLOF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check PyTorch version and CUDA availability\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKnWLKfGRWRL",
        "outputId": "f6f36a82-eb46-4069-a04c-17b6f1a36fc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.6.0+cu124\n",
            "CUDA available: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hugging Face Authentication\n",
        "\n",
        "LLaMa 3.2 requires authentication with Hugging Face to access the model. You'll need to:\n",
        "1. Have a Hugging Face account\n",
        "2. Accept the LLaMa 3.2 model terms of use on the Hugging Face model page\n",
        "3. Create an access token on Hugging Face (https://huggingface.co/settings/tokens)\n",
        "\n",
        "After you have your access token and have accepted the terms, the code below will help you log in:"
      ],
      "metadata": {
        "id": "BSe7ErY8Ur6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "import getpass\n",
        "\n",
        "token = getpass.getpass(\"Enter your Hugging Face token: \")\n",
        "login(token=token)\n",
        "\n",
        "# Verify login\n",
        "print(\"Login status: Authenticated with Hugging Face\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNKjESjjUqR-",
        "outputId": "596179ce-3653-4ebc-ad53-a282f40eac29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Hugging Face token: ··········\n",
            "Login status: Authenticated with Hugging Face\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer Setup and Exploration\n",
        "\n",
        "LLaMa 3.2 uses a sophisticated tokenizer that supports multiple languages. Understanding how the tokenizer works is crucial for:\n",
        "- Preparing training data effectively\n",
        "- Managing sequence lengths\n",
        "- Understanding model behavior across languages\n",
        "\n",
        "Let's load the tokenizer and explore its basic properties:"
      ],
      "metadata": {
        "id": "qZEfRS18RVf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer\n",
        "model_id = \"meta-llama/Llama-3.2-1B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Make sure padding token is set\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Basic tokenizer information\n",
        "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
        "print(f\"Model max length: {tokenizer.model_max_length}\")\n",
        "print(f\"Padding token: {tokenizer.pad_token}\")\n",
        "print(f\"End of sequence token: {tokenizer.eos_token}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5S_rxI9aUMu1",
        "outputId": "0c25fec3-420e-4dba-f03a-c897a5b2d17c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 128256\n",
            "Model max length: 131072\n",
            "Padding token: <|end_of_text|>\n",
            "End of sequence token: <|end_of_text|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding Tokenization\n",
        "\n",
        "Let's see how the tokenizer processes text in different languages. This will help us understand:\n",
        "- How words are broken into tokens\n",
        "- How special characters are handled\n",
        "- Token counts for different languages"
      ],
      "metadata": {
        "id": "VBJpzuBEUYXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example texts in different languages\n",
        "texts = {\n",
        "    \"English\": \"Hello, how are you today?\",\n",
        "    \"Spanish\": \"¡Hola! ¿Cómo estás hoy?\",\n",
        "    \"French\": \"Bonjour! Comment allez-vous aujourd'hui?\",\n",
        "    \"German\": \"Hallo! Wie geht es dir heute?\"\n",
        "}\n",
        "\n",
        "# Analyze tokenization for each language\n",
        "for lang, text in texts.items():\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    token_ids = tokenizer.encode(text)\n",
        "\n",
        "    print(f\"\\n{lang}:\")\n",
        "    print(f\"Original text: {text}\")\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "    print(f\"Number of tokens: {len(tokens)}\")\n",
        "    print(f\"Token IDs: {token_ids}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEYn8lFlhJNu",
        "outputId": "5da9cd9a-a716-4979-f1b0-d0707539838a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "English:\n",
            "Original text: Hello, how are you today?\n",
            "Tokens: ['Hello', ',', 'Ġhow', 'Ġare', 'Ġyou', 'Ġtoday', '?']\n",
            "Number of tokens: 7\n",
            "Token IDs: [128000, 9906, 11, 1268, 527, 499, 3432, 30]\n",
            "\n",
            "Spanish:\n",
            "Original text: ¡Hola! ¿Cómo estás hoy?\n",
            "Tokens: ['Â¡', 'Hola', '!', 'ĠÂ¿', 'CÃ³mo', 'Ġest', 'Ã¡s', 'Ġhoy', '?']\n",
            "Number of tokens: 9\n",
            "Token IDs: [128000, 40932, 69112, 0, 29386, 96997, 1826, 7206, 49841, 30]\n",
            "\n",
            "French:\n",
            "Original text: Bonjour! Comment allez-vous aujourd'hui?\n",
            "Tokens: ['Bonjour', '!', 'ĠComment', 'Ġalle', 'z', '-vous', 'Ġaujourd', \"'hui\", '?']\n",
            "Number of tokens: 9\n",
            "Token IDs: [128000, 82681, 0, 12535, 12584, 89, 45325, 75804, 88253, 30]\n",
            "\n",
            "German:\n",
            "Original text: Hallo! Wie geht es dir heute?\n",
            "Tokens: ['Hallo', '!', 'ĠWie', 'Ġgeht', 'Ġes', 'Ġdir', 'Ġheute', '?']\n",
            "Number of tokens: 8\n",
            "Token IDs: [128000, 79178, 0, 43716, 40364, 1560, 5534, 49714, 30]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Notice how:\n",
        "1. Punctuations are their own tokens\n",
        "2. The 'Ġ' symbol represents a space before the token\n",
        "3. Some words are single tokens (like 'today') while others might be split"
      ],
      "metadata": {
        "id": "iLfE92eUwMOg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation for Fine-tuning\n",
        "\n",
        "This notebook demonstrates how to update LLaMa 3.2's knowledge base with domain-specific information. This type of fine-tuning can help the model:\n",
        "- Learn new domain-specific facts and concepts\n",
        "- Update its knowledge about specific topics\n",
        "- Improve its ability to discuss specialized subjects\n",
        "\n",
        "For domain knowledge fine-tuning, we'll use the simplest appraoch of direct text chunks: the model will learn directly from the source material."
      ],
      "metadata": {
        "id": "84ivn4uql6rM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Structuring the Data\n",
        "\n",
        "Fine-tuning LLaMa 3.2 requires carefully formatted training data. The model expects:\n",
        "- Input text in a specific format\n",
        "- Response text that follows the input\n",
        "- Proper formatting of system prompts and chat turns\n",
        "\n",
        "We'll set up our dataset as such in this section."
      ],
      "metadata": {
        "id": "xA0Tm_5oSm9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distinct pieces of data for the model are relatively small text segments. So here, we'll start by setting up a function to create chunks of text from longer, full documents for the model to train on.\n",
        "\n",
        "### Important Parameters:\n",
        "- `chunk_size`: Default 512 tokens. Can be adjusted based on your GPU memory and needs\n",
        "- `tokenizer`: Uses LLaMa's tokenizer to ensure proper text splitting\n"
      ],
      "metadata": {
        "id": "0FftSeZbmxeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_text_chunks(document: str, tokenizer, chunk_size: int = 512) -> List[str]:\n",
        "    \"\"\"\n",
        "    Create chunks of text that:\n",
        "    - Maintain sentence boundaries where possible\n",
        "    - Have a reasonable minimum size\n",
        "    - Don't include special tokens\n",
        "    \"\"\"\n",
        "    # Clean the text first\n",
        "    document = document.strip().replace('\\n', ' ')\n",
        "\n",
        "    # Split into sentences\n",
        "    sentences = [s.strip() + '.' for s in document.split('.') if s.strip()]\n",
        "\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Get token count for this sentence\n",
        "        tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
        "        sentence_length = len(tokens)\n",
        "\n",
        "        if current_length + sentence_length > chunk_size:\n",
        "            if current_chunk:\n",
        "                # Join the current chunk and add it to chunks\n",
        "                chunk_text = ' '.join(current_chunk)\n",
        "                chunks.append(chunk_text)\n",
        "                # Start new chunk with current sentence\n",
        "                current_chunk = [sentence]\n",
        "                current_length = sentence_length\n",
        "        else:\n",
        "            current_chunk.append(sentence)\n",
        "            current_length += sentence_length\n",
        "\n",
        "    # Don't forget the last chunk\n",
        "    if current_chunk:\n",
        "        chunk_text = ' '.join(current_chunk)\n",
        "        chunks.append(chunk_text)\n",
        "\n",
        "    # Print some diagnostics\n",
        "    total_tokens = sum(len(tokenizer.encode(chunk, add_special_tokens=False))\n",
        "                      for chunk in chunks)\n",
        "\n",
        "    print(f\"Created {len(chunks)} chunks with total {total_tokens} tokens\")\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "MUVdp5zomw4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use an example chunk of text to see how this works before proceeding."
      ],
      "metadata": {
        "id": "-ommRhmUz0xe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example document - we'll replace this with actual domain content later\n",
        "sample_document = \"\"\"\n",
        "LLMs process text using attention mechanisms. These mechanisms allow the model\n",
        "to weigh different parts of the input differently. The transformer architecture\n",
        "revolutionized natural language processing. It introduced self-attention as a\n",
        "core component. Modern language models build upon this foundation. As such, LLM's\n",
        "are able to process sentences similar to the way that humans do, by understanding\n",
        "words in the sentence relative to those around them.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "print(\"\\nTesting with chunk_size=20:\")\n",
        "chunks = create_text_chunks(sample_document, tokenizer, chunk_size=20)\n",
        "for i, chunk in enumerate(chunks[0:2]):\n",
        "    tokens = tokenizer.tokenize(chunk)\n",
        "    print(f\"\\nChunk {i+1}:\")\n",
        "    print(f\"Text: {chunk}\")\n",
        "    print(f\"Token count: {len(tokens)}\")\n",
        "    print(f\"Tokens: {tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEgHXy90unMV",
        "outputId": "64332a1d-d6a0-4c1d-e7a9-a79c06f8bb0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing with chunk_size=20:\n",
            "Created 5 chunks with total 80 tokens\n",
            "\n",
            "Chunk 1:\n",
            "Text: LLMs process text using attention mechanisms.\n",
            "Token count: 8\n",
            "Tokens: ['LL', 'Ms', 'Ġprocess', 'Ġtext', 'Ġusing', 'Ġattention', 'Ġmechanisms', '.']\n",
            "\n",
            "Chunk 2:\n",
            "Text: These mechanisms allow the model to weigh different parts of the input differently.\n",
            "Token count: 14\n",
            "Tokens: ['These', 'Ġmechanisms', 'Ġallow', 'Ġthe', 'Ġmodel', 'Ġto', 'Ġweigh', 'Ġdifferent', 'Ġparts', 'Ġof', 'Ġthe', 'Ġinput', 'Ġdifferently', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and Processing PDF Documents\n",
        "\n",
        "We'll be creating these chunks from text documents.\n",
        "\n",
        "In reality, we'll want to accommodate giving some PDF's as a document set to fine-tune on. We'll use pdfminer to read PDFs and extract their text content. Then we'll process this text into appropriate chunks for training."
      ],
      "metadata": {
        "id": "GcyMjC7pxBpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdfs(pdf_paths: List[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Extract text from multiple PDF files.\n",
        "\n",
        "    Args:\n",
        "        pdf_paths: List of paths to PDF files\n",
        "\n",
        "    Returns:\n",
        "        List of extracted text documents\n",
        "    \"\"\"\n",
        "    all_texts = []\n",
        "\n",
        "    for path in pdf_paths:\n",
        "        try:\n",
        "            text = extract_text(path)\n",
        "            all_texts.append(text)\n",
        "            print(f\"Successfully processed: {path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {path}: {str(e)}\")\n",
        "\n",
        "    return all_texts"
      ],
      "metadata": {
        "id": "KJB6_EvXmCdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Academic papers often contain a lot of irregular, non-text components. Between images, captions, references, etc, there's a lot that can confuse the model if we aren't careful to remove it. So here, we'll write a function that cleans our text after we read it, attempting to remove as many of these artifacts as possible."
      ],
      "metadata": {
        "id": "_sMc-MNZ9Csa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_academic_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Clean academic text with improved word separation.\n",
        "    \"\"\"\n",
        "    if not text or not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    original_length = len(text)\n",
        "\n",
        "    # First clean pass - basic normalization\n",
        "    text = text.strip()\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
        "\n",
        "    # Fix incorrectly joined words\n",
        "    # Look for patterns like \"wordWord\" or \"word-Word\"\n",
        "    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)  # Split \"wordWord\"\n",
        "    text = re.sub(r'([a-z])-\\s*([a-z])', r'\\1\\2', text)  # Join \"wo rd\"\n",
        "    text = re.sub(r'\\s*-\\s*', '-', text)  # Clean up hyphens\n",
        "\n",
        "    # Fix common academic text patterns\n",
        "    patterns = {\n",
        "        r'(?<=\\w)\\.(?=\\w)': '. ',  # Add space after period between words\n",
        "        r'(?<=\\w)\\s+\\(': ' (',      # Fix spacing around parentheses\n",
        "        r'\\)\\s+(?=\\w)': ') ',\n",
        "        r'(?<=\\d),(?=\\d)': ', ',    # Add space after comma between numbers\n",
        "        r'(?<=[a-z])(?=\\d)': ' ',   # Add space between letters and numbers\n",
        "    }\n",
        "\n",
        "    for pattern, replacement in patterns.items():\n",
        "        text = re.sub(pattern, replacement, text)\n",
        "\n",
        "    # Remove common PDF artifacts\n",
        "    artifacts = [\n",
        "        r'Fig\\.\\s*\\d+',\n",
        "        r'Figure\\s*\\d+:?.*?\\n',\n",
        "        r'Table\\s*\\d+:?.*?\\n',\n",
        "        r'\\[\\d+(?:,\\s*\\d+)*\\]',     # Citations [1] or [1,2,3]\n",
        "        r'\\(\\w+\\s+et\\s+al\\.,\\s+\\d{4}\\)',  # Citations (Author et al., 2020)\n",
        "        r'References.*$',            # Remove references section\n",
        "        r'Bibliography.*$',\n",
        "    ]\n",
        "\n",
        "    for pattern in artifacts:\n",
        "        text = re.sub(pattern, ' ', text)\n",
        "\n",
        "    # Final cleanup\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Normalize spaces again\n",
        "    text = text.strip()\n",
        "\n",
        "    # Print sample before/after for verification\n",
        "    print(\"\\nSample text cleaning comparison:\")\n",
        "    print(\"Original first 100 chars:\", text[:100])\n",
        "    sample_cleaned = text[:100]\n",
        "    print(\"Cleaned first 100 chars:\", sample_cleaned)\n",
        "\n",
        "    final_length = len(text)\n",
        "    removed = original_length - final_length\n",
        "    if removed > 0:\n",
        "        print(f\"\\nRemoved {removed} characters ({removed/original_length*100:.1f}% of original text)\")\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "vN77Yo509UIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Formatting Text Chunks for LLaMa\n",
        "\n",
        "Finally, we need to properly format our text chunks into a format that the model expects and understands.\n",
        "\n",
        "Each chunk will be formatted as:\n",
        "\n",
        "`<|system|>Learn the following information: </s><|user|>{cleaned_text}</s><|assistant|>I understand this information.</s>`\n",
        "\n",
        "This format follows LLaMa's chat template structure, which is crucial because:\n",
        "\n",
        "1. LLaMa 3.2 was trained using a specific chat format with different roles:\n",
        "   - `<|system|>`: Provides context or instructions to the model\n",
        "   - `<|user|>`: Represents input content\n",
        "   - `</s>`: Special token marking the end of each turn\n",
        "\n",
        "2. The model expects input in the same format it was originally trained on, so using a different format might confuse the model or reduce learning effectiveness"
      ],
      "metadata": {
        "id": "VxRsJvm5TYED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_training_example(text: str) -> str:\n",
        "    \"\"\"Format text for training\"\"\"\n",
        "    cleaned_text = text.strip()\n",
        "    if not cleaned_text:  # Skip empty chunks\n",
        "        return None\n",
        "    return f\"<|system|>Learn the following information: </s><|user|>{cleaned_text}</s><|assistant|>I understand this information.</s>\""
      ],
      "metadata": {
        "id": "iRr0E2oDTbJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PDF's to Training Data Pipeline\n",
        "\n",
        "We'll use these above functions to now build a pipeline for converting PDF documents into a format suitable for training LLaMa 3.2. The process involves:\n",
        "1. Loading PDFs and extracting text\n",
        "2. Cleaning and preprocessing the text\n",
        "3. Chunking the text into appropriate sizes\n",
        "4. Creating a dataset for training"
      ],
      "metadata": {
        "id": "zDcmjOwl1aea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "def create_training_dataset(pdf_directory, tokenizer, chunk_size=512):\n",
        "    \"\"\"Create a training dataset from PDF documents\"\"\"\n",
        "\n",
        "    # First get all PDFs and their text\n",
        "    pdf_paths = [f\"{pdf_directory}/{f}\" for f in os.listdir(pdf_directory)\n",
        "                if f.endswith('.pdf')]\n",
        "\n",
        "    print(f\"Found {len(pdf_paths)} PDFs\")\n",
        "\n",
        "    # Extract text from PDFs\n",
        "    documents = extract_text_from_pdfs(pdf_paths)\n",
        "    print(f\"Extracted text from {len(documents)} documents\")\n",
        "\n",
        "    all_chunks = []\n",
        "\n",
        "    for doc in documents:\n",
        "        # Clean text first\n",
        "        text = clean_academic_text(doc)\n",
        "        # Create chunks from the document\n",
        "        chunks = create_text_chunks(text, tokenizer, chunk_size)\n",
        "\n",
        "        # Format each chunk\n",
        "        formatted_chunks = [format_training_example(chunk) for chunk in chunks]\n",
        "        all_chunks.extend(formatted_chunks)\n",
        "    print(\"\\n\")\n",
        "    print(f\"\\033[1m\\033[91mCreated {len(all_chunks)} total chunks\\033[0m\")\n",
        "\n",
        "    # Create dataset dictionary\n",
        "    dataset_dict = {\n",
        "        \"input_ids\": [],\n",
        "        \"attention_mask\": [],\n",
        "        \"labels\": []\n",
        "    }\n",
        "\n",
        "    # Process each chunk\n",
        "    for chunk in all_chunks:\n",
        "        encodings = tokenizer(\n",
        "            chunk,\n",
        "            truncation=True,\n",
        "            max_length=chunk_size,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        dataset_dict[\"input_ids\"].append(encodings[\"input_ids\"].squeeze().numpy())  # Convert to numpy\n",
        "        dataset_dict[\"attention_mask\"].append(encodings[\"attention_mask\"].squeeze().numpy())\n",
        "        dataset_dict[\"labels\"].append(encodings[\"input_ids\"].squeeze().numpy())\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    for key in dataset_dict:\n",
        "        if dataset_dict[key]:  # Check if the list is not empty\n",
        "            dataset_dict[key] = np.array(dataset_dict[key])\n",
        "\n",
        "    return Dataset.from_dict(dataset_dict)"
      ],
      "metadata": {
        "id": "6gBRGY_O2__7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uploading our Docs + Making the Dataset\n",
        "\n",
        "Now, all that's left is to create a directory of pdf's that we can point the dataset generation code to.\n",
        "\n",
        "Below, you can upload a set of PDF's that will become the training data.\n",
        "\n",
        "This will be automatically saved to a path called training_pdfs, which we'll use for setting up our dataset in the following cells."
      ],
      "metadata": {
        "id": "g7vW7m-9031m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Create a directory for PDFs\n",
        "!mkdir -p training_pdfs\n",
        "\n",
        "print(\"Please upload your PDF files using the file uploader that appears below:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Move uploaded files to our PDF directory\n",
        "for filename in uploaded.keys():\n",
        "    os.rename(filename, f\"training_pdfs/{filename}\")\n",
        "    print(f\"Moved {filename} to training_pdfs/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "Z45yxu0h0CSH",
        "outputId": "4ab28560-2d3b-467c-e56e-b723cb0ff72e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your PDF files using the file uploader that appears below:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1ad226ce-c9f3-4d45-90bc-1de34eea5609\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1ad226ce-c9f3-4d45-90bc-1de34eea5609\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving LENA-Foundations-of-Literacy_Webinar-Slides.pdf to LENA-Foundations-of-Literacy_Webinar-Slides.pdf\n",
            "Saving Lenhart & Lingel (2023) ECRQ.pdf to Lenhart & Lingel (2023) ECRQ.pdf\n",
            "Saving Wexler on conversations.pdf to Wexler on conversations.pdf\n",
            "Moved LENA-Foundations-of-Literacy_Webinar-Slides.pdf to training_pdfs/\n",
            "Moved Lenhart & Lingel (2023) ECRQ.pdf to training_pdfs/\n",
            "Moved Wexler on conversations.pdf to training_pdfs/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And putting it togehter:"
      ],
      "metadata": {
        "id": "kZY1zS3EUt9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset\n",
        "training_dataset = create_training_dataset(\"training_pdfs\", tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xzu2b3O25vQZ",
        "outputId": "4702916d-224e-4221-f53a-2486168d0305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3 PDFs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully processed: training_pdfs/Lenhart & Lingel (2023) ECRQ.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully processed: training_pdfs/LENA-Foundations-of-Literacy_Webinar-Slides.pdf\n",
            "Successfully processed: training_pdfs/Wexler on conversations.pdf\n",
            "Extracted text from 3 documents\n",
            "\n",
            "Sample text cleaning comparison:\n",
            "Original first 100 chars: Early Childhood Research Quarterly 64 (2023) 119–128 Contents lists available at Science Direct Earl\n",
            "Cleaned first 100 chars: Early Childhood Research Quarterly 64 (2023) 119–128 Contents lists available at Science Direct Earl\n",
            "\n",
            "Removed 16396 characters (22.3% of original text)\n",
            "Created 30 chunks with total 15642 tokens\n",
            "\n",
            "Sample text cleaning comparison:\n",
            "Original first 100 chars: Foundations of Literacy: The Science of Reading November 2, 2023 Our Mission LENA is a national nonp\n",
            "Cleaned first 100 chars: Foundations of Literacy: The Science of Reading November 2, 2023 Our Mission LENA is a national nonp\n",
            "\n",
            "Removed 599 characters (10.5% of original text)\n",
            "Created 3 chunks with total 1150 tokens\n",
            "\n",
            "Sample text cleaning comparison:\n",
            "Original first 100 chars: Everyday Conversations How They Help Your Child Become a Strong Reader By Natalie Wexler Me: Well, m\n",
            "Cleaned first 100 chars: Everyday Conversations How They Help Your Child Become a Strong Reader By Natalie Wexler Me: Well, m\n",
            "\n",
            "Removed 171 characters (2.1% of original text)\n",
            "Created 4 chunks with total 1686 tokens\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[91mCreated 37 total chunks\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Dataset Size\n",
        "\n",
        "The code above will tell you how many total chunks were generated from the PDF's that you uploaded. When fine-tuning LLaMa for domain knowledge, the number of training chunks is crucial:\n",
        "\n",
        "### Recommended Chunk Numbers:\n",
        "- **Minimum**: 200-300 chunks\n",
        " - Below this, the model may not learn effectively\n",
        " - Risk of overfitting to limited examples\n",
        "\n",
        "- **Target**: 500-1000+ chunks\n",
        " - Provides enough examples for robust learning\n",
        " - Allows for diverse phrasings of similar concepts\n",
        "\n",
        "- **Creating More Chunks**:\n",
        " - Add more domain documents\n",
        " - Use overlapping chunks (e.g., 50-token overlap)\n",
        " - Include related papers/documents\n",
        " - Consider smaller chunk sizes (but not below 256 tokens)"
      ],
      "metadata": {
        "id": "XIyHdLxcUwjT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training LLaMa 3.2 on Domain Knowledge\n",
        "\n"
      ],
      "metadata": {
        "id": "BMTehEWYLgXq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Configuration\n",
        "\n",
        "For fine-tuning LLaMa 3.2, we need to carefully configure several parameters:\n",
        "- Training precision (using bfloat16 for efficiency)\n",
        "- Memory optimization settings\n",
        "- Model configuration parameters\n",
        "\n",
        "We'll start with a basic configuration that works well on Google Colab, but these parameters can be adjusted based on your specific needs and hardware capabilities."
      ],
      "metadata": {
        "id": "-dWtHxoBUic2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, TrainingArguments\n",
        "\n",
        "# Reload model without 8-bit quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "print(\"Model configuration:\")\n",
        "print(f\"Number of parameters: {model.num_parameters():,}\")\n",
        "print(f\"Training device: {model.device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhpccGhdUZ2x",
        "outputId": "ecbca2e9-1436-46a5-bb5d-ebf52293b9fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model configuration:\n",
            "Number of parameters: 1,235,814,400\n",
            "Training device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Setup and Configuration\n",
        "\n",
        "We'll configure training with:\n",
        "- A relatively small number of epochs since we're fine-tuning\n",
        "- Gradient accumulation to handle larger effective batch sizes\n",
        "- Learning rate with warmup\n",
        "\n",
        "#### Key Training Parameters:\n",
        "- `learning_rate=3e-4`: Higher than typical fine-tuning to allow new knowledge absorption\n",
        "- `per_device_train_batch_size=4`: Adjust based on your GPU memory\n",
        "- `gradient_accumulation_steps=8`: Effectively creates batch_size of 32\n",
        "- `num_train_epochs=3`: Adjust based on dataset size and convergence"
      ],
      "metadata": {
        "id": "QHL0xBEnLi_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using Different Model Sizes:\n",
        "\n",
        "This setup can be adapted for different LLaMa 3.2 sizes (1B, 3B, etc.) and different hardware:\n",
        "\n",
        "To use a larger LLaMa model, simply change the model ID:\n",
        "```\n",
        "# For 1B model (current)\n",
        "model_id = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "# For 3B model\n",
        "# model_id = \"meta-llama/Llama-3.2-3B\"\n",
        "```\n",
        "\n",
        "Hardware Considerations:\n",
        "- 1B model: Runs on most GPUs with 16GB+ memory\n",
        "- 3B model: Recommended 24GB+ GPU memory\n",
        "- For smaller GPUs: Reduce batch size and increase gradient accumulation\n",
        "- For larger GPUs: Increase batch size for faster training"
      ],
      "metadata": {
        "id": "3lBKG9GLV_d5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also set up. regular checkpoints to save progress.\n",
        "\n",
        "We can set up progress monitoring using wandb (Weights & Biases), which is a popular tool for tracking machine learning experiments. It creates nice visualizations of your training metrics (like loss over time), GPU usage, etc. When you run ML training, it sends the data to their website where you can view it in nice dashboards. If you want to use it, you'll need a wandb API key.\n",
        "\n",
        "Iif you don't want to use it, you can set the `use_wandb` parameter at the top of this next cell to = `False`"
      ],
      "metadata": {
        "id": "CStSBrmLWcNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_wandb = False\n",
        "\n",
        "# Clear any existing accelerator state\n",
        "AcceleratorState._reset_state()\n",
        "\n",
        "# Initialize accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "report_to = \"none\"\n",
        "if use_wandb:\n",
        "  # Initialize wandb\n",
        "  wandb.init(\n",
        "      project=\"llama-domain-training\",\n",
        "      name=\"domain-knowledge-run\",\n",
        "      config={\n",
        "          \"model\": \"LLaMa-3.2-1B\",\n",
        "          \"dataset_size\": len(training_dataset),\n",
        "          \"chunk_size\": 512\n",
        "      }\n",
        "  )\n",
        "  report_to = 'wandb'\n",
        "\n",
        "# Training arguments with accelerator config\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./domain_trained_model\",\n",
        "    learning_rate=1e-4,              # Small learning rate\n",
        "    per_device_train_batch_size=4,   # Start smaller, we can adjust\n",
        "    gradient_accumulation_steps=8,   # Add gradient accumulation\n",
        "    num_train_epochs=50,\n",
        "    bf16=True,                      # Enable mixed precision\n",
        "    logging_steps=1,                # Log every step so we can monitor\n",
        "    save_strategy=\"epoch\",\n",
        "    optim=\"adamw_8bit\",            # Use 8-bit optimizer\n",
        "    weight_decay=0.01,             # Add weight decay\n",
        "    warmup_steps=10                # Add warmup steps\n",
        ")"
      ],
      "metadata": {
        "id": "sCTmLl7o2f89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) PEFT Configuration\n",
        "\n",
        "Parameter Efficient Fine-Tuning (PEFT) lets us fine-tune LLaMA using much less memory. We'll use LoRA (Low-Rank Adaptation), which is particularly effective for LLMs."
      ],
      "metadata": {
        "id": "P2puurnJqF-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Toggle for full fine-tuning\n",
        "use_peft = True\n",
        "\n",
        "if use_peft:\n",
        "    # LoRA configuration\n",
        "    peft_config = LoraConfig(\n",
        "        r=16,                     # Rank of update matrices\n",
        "        lora_alpha=32,           # Alpha parameter for LoRA scaling\n",
        "        lora_dropout=0.05,       # Dropout probability for LoRA layers\n",
        "        target_modules=[         # Which modules to apply LoRA to\n",
        "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "            \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "        ],\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\"    # For causal language modeling\n",
        "    )\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    model.print_trainable_parameters()  # Shows % of parameters being trained"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_eWEkLWqFV0",
        "outputId": "b8d9cb08-14ed-4260-df16-71242dc52332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:bitsandbytes.cextension:The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 11,272,192 || all params: 1,247,086,592 || trainable%: 0.9039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop\n",
        "\n",
        "Now we'll set up the trainer and start training. We'll include:\n",
        "- A simple progress callback to monitor training\n",
        "- Basic error handling\n",
        "- Checkpoint saving"
      ],
      "metadata": {
        "id": "ayhnUHrFMBzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainerCallback\n",
        "\n",
        "class ProgressCallback(TrainerCallback):\n",
        "    \"\"\"Simple callback to print progress during training\"\"\"\n",
        "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
        "        print(f\"\\nStarting epoch {state.epoch + 1}/{args.num_train_epochs}\")\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs:\n",
        "            # Handle the loss value more carefully\n",
        "            loss = logs.get('loss', 'N/A')\n",
        "            if isinstance(loss, (float, int)):\n",
        "                print(f\"Step {state.global_step}: Loss = {loss:.4f}\")\n",
        "            else:\n",
        "                print(f\"Step {state.global_step}: Loss = {loss}\")\n",
        "\n",
        "# Set up trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=training_dataset,\n",
        "    callbacks=[ProgressCallback()]\n",
        ")"
      ],
      "metadata": {
        "id": "5WzBd5OpMCib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "243a2928-bede-4be6-d4d3-a1ef60ed5a29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Monitoring Training\n",
        "\n",
        "You can monitor the training in several ways:\n",
        "1. Direct console output showing loss every 10 steps\n",
        "2. Weights & Biases dashboard (wandb.ai) showing:\n",
        "   - Training loss over time\n",
        "   - Learning rate schedule\n",
        "   - GPU memory usage\n",
        "   - Training speed\n",
        "3. Model checkpoints saved after each epoch\n",
        "\n",
        "The training may take some time depending on your GPU. You'll see regular updates on:\n",
        "- Current epoch\n",
        "- Current step\n",
        "- Loss value\n",
        "- Any potential issues"
      ],
      "metadata": {
        "id": "N4X2jsrqMNZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop with detailed error tracking\n",
        "try:\n",
        "    print(\"Starting training...\")\n",
        "\n",
        "    # Track where we are in training\n",
        "    current_step = 0\n",
        "    try:\n",
        "        trainer_output = trainer.train()\n",
        "    except Exception as train_error:\n",
        "        print(\"\\nError during trainer.train():\")\n",
        "        print(f\"Step when error occurred: {current_step}\")\n",
        "        print(f\"Error type: {type(train_error)}\")\n",
        "        print(f\"Error message: {str(train_error)}\")\n",
        "        # Print the full error traceback\n",
        "        import traceback\n",
        "        print(\"\\nFull error traceback:\")\n",
        "        print(traceback.format_exc())\n",
        "        raise  # Re-raise the error to see full stack trace\n",
        "\n",
        "    print(\"\\nTraining completed!\")\n",
        "\n",
        "    # Save the final model\n",
        "    trainer.save_model(\"./final_model\")\n",
        "    print(\"Model saved to ./final_model\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Final error catch:\", str(e))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "HLDgRjS3MMdH",
        "outputId": "9c597d92-d0f3-437c-9ce8-e563de3066ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabigail-petulante\u001b[0m (\u001b[33mabigail-petulante-vanderbilt-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250519_153555-57s90r3j</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/abigail-petulante-vanderbilt-university/huggingface/runs/57s90r3j' target=\"_blank\">./domain_trained_model</a></strong> to <a href='https://wandb.ai/abigail-petulante-vanderbilt-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/abigail-petulante-vanderbilt-university/huggingface' target=\"_blank\">https://wandb.ai/abigail-petulante-vanderbilt-university/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/abigail-petulante-vanderbilt-university/huggingface/runs/57s90r3j' target=\"_blank\">https://wandb.ai/abigail-petulante-vanderbilt-university/huggingface/runs/57s90r3j</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:3700: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  ctx_manager = torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking our Fine-Tuning Results!\n",
        "\n",
        "If the above cell ran, then congrats! You've fine-tuned a LLaMa 3.2 1B model on some new domain knowledge! Let's test that it actually learned what we wanted it to.\n",
        "\n",
        "We'll test the model's knowledge by:\n",
        "1. Asking domain-specific questions\n",
        "2. Comparing responses between original and fine-tuned models\n",
        "3. Looking for improvements in accuracy and detail"
      ],
      "metadata": {
        "id": "UjcoP3JdQFk6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's just confirm that we've *actually* changed the model."
      ],
      "metadata": {
        "id": "7aDrQlPxrGBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-3.2-1B\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "fine_tuned_path =\"./final_model\"\n",
        "\n",
        "if use_peft:\n",
        "    fine_tuned_path = \"./final_model\"\n",
        "    config = PeftConfig.from_pretrained(fine_tuned_path)\n",
        "    fine_tuned_model = PeftModel.from_pretrained(\n",
        "        base_model,\n",
        "        fine_tuned_path,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    # To compare PEFT models, we need to look at the LoRA weights\n",
        "    print(\"\\nLoRA Adapter Information:\")\n",
        "    for name, param in fine_tuned_model.named_parameters():\n",
        "        if 'lora' in name:  # Only look at LoRA parameters\n",
        "            print(f\"Found LoRA weights in {name}\")\n",
        "            print(f\"Non-zero parameters: {torch.sum(param != 0).item()}\")\n",
        "\n",
        "    # The base parameters should be identical (that's the point of PEFT!)\n",
        "    print(\"\\nBase parameters are identical?:\", torch.allclose(\n",
        "        next(base_model.parameters()),\n",
        "        next(fine_tuned_model.base_model.parameters())\n",
        "    ))\n",
        "else:\n",
        "    fine_tuned_model = AutoModelForCausalLM.from_pretrained(\n",
        "        fine_tuned_path,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    # Compare some weights to see if they're actually different\n",
        "    base_params = next(base_model.parameters())\n",
        "    fine_tuned_params = next(fine_tuned_model.parameters())\n",
        "\n",
        "    print(\"Are the models identical?\", torch.allclose(base_params, fine_tuned_params))"
      ],
      "metadata": {
        "id": "hHnEgQhItepe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also double-check that the training generated the outputs that we expect."
      ],
      "metadata": {
        "id": "lpSXp7xdrKv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at training output directory\n",
        "print(\"Training output contents:\")\n",
        "if os.path.exists(\"./domain_trained_model\"):\n",
        "    print(\"\\ndomain_trained_model directory contains:\")\n",
        "    for item in os.listdir(\"./domain_trained_model\"):\n",
        "        print(f\"- {item}\")\n",
        "        if os.path.isdir(f\"./domain_trained_model/{item}\"):\n",
        "            print(f\"  Contains: {os.listdir(f'./domain_trained_model/{item}')}\")\n",
        "\n",
        "# If we have a trainer_state.json, let's look at it\n",
        "import json\n",
        "if os.path.exists(\"./domain_trained_model/checkpoint-3/trainer_state.json\"):\n",
        "    with open(\"./domain_trained_model/checkpoint-3/trainer_state.json\", 'r') as f:\n",
        "        state = json.load(f)\n",
        "    print(\"\\nTraining history:\")\n",
        "    print(state.get('log_history', []))"
      ],
      "metadata": {
        "id": "nvkYZFjdlV5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we'll set up a function to query our model with a question."
      ],
      "metadata": {
        "id": "9yR1k4y-QZ7W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing Original vs Fine-Tuned Models\n",
        "\n",
        "Now, we'll compare the performance of our fine-tuned model vs. the original model on some questions that are meant to test the model's domain knowledge."
      ],
      "metadata": {
        "id": "GDLmJEyzRfwz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Questions\n",
        "\n",
        "Let's define some questions to test our models' knowledge. These should be specific to your PDF content. These questions are for a dataset of PDF's that discuss \"dialogic reading\""
      ],
      "metadata": {
        "id": "ku1hcnOTQwWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create test questions based on PDF content\n",
        "test_questions = [\n",
        "    \"Who developed the concept of dialogic reading\",\n",
        "    \"What ages is dialogic reading appropriate for?\",\n",
        "    \"What are dialogic reading prompt types?\",\n",
        "]"
      ],
      "metadata": {
        "id": "Z0qDaogCQuZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Original Model\n",
        "\n",
        "First, let's look at how LLaMa 3.2 performs on these questions out of the box, before training to gain any additional knowledge."
      ],
      "metadata": {
        "id": "ELI90f0QL9KV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pipeline with LLaMA 3.2 original\n",
        "pipe_original = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"meta-llama/Llama-3.2-1B\",\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "# Test each question\n",
        "print(\"Testing original model responses:\")\n",
        "print(\"-\" * 50)\n",
        "for question in test_questions:\n",
        "    print(f\"\\nQ: {question}\")\n",
        "    result = pipe_original(question, max_length=100)\n",
        "    print(f\"A: {result[0]['generated_text']}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "oYZTQAfYT6gI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Our Fine-Tuned Model\n",
        "\n",
        "Now, we give those same questions to our model, which we've fine tuned for additional domain knowledge. We'll look for signs that the domain knowledge has been absorbed by the model by how it answers the question."
      ],
      "metadata": {
        "id": "yCpy4-HNMLc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pipeline with our fine-tuned model\n",
        "\n",
        "if use_peft:\n",
        "  peft_model = PeftModel.from_pretrained(\n",
        "      base_model,\n",
        "      fine_tuned_path,\n",
        "      torch_dtype=torch.float16,\n",
        "      device_map=\"auto\"\n",
        "  )\n",
        "\n",
        "  # Get the merged model\n",
        "  fine_tuned_model = peft_model.merge_and_unload()  # This combines PEFT and base weights\n",
        "\n",
        "  pipe_finetune = pipeline(\n",
        "      \"text-generation\",\n",
        "      model=fine_tuned_model,  # base model + peft weights\n",
        "      tokenizer=tokenizer,\n",
        "      torch_dtype=torch.float16,\n",
        "      device_map=\"auto\"\n",
        "  )\n",
        "\n",
        "else:\n",
        "  pipe_finetune = pipeline(\n",
        "      \"text-generation\",\n",
        "      model=fine_tuned_path,  # Path to our saved fine-tuned model\n",
        "      tokenizer=tokenizer,\n",
        "      torch_dtype=torch.float16,\n",
        "      device_map=\"auto\"\n",
        "  )\n",
        "# Test each question\n",
        "print(\"Testing fine-tuned model responses:\")\n",
        "print(\"-\" * 50)\n",
        "for question in test_questions:\n",
        "    print(f\"\\nQ: {question}\")\n",
        "    result = pipe_finetune(question, max_length=100)\n",
        "    print(f\"A: {result[0]['generated_text']}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "jYKSBMDyfedt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking General Knowledge Retention\n",
        "\n",
        "Often times for very small models, training them to gain specific domain knowledge leads to forgetting existing knowledge.\n",
        "\n",
        "Here, we'll check a few basic knowledge questions to see if the model has retained understanding."
      ],
      "metadata": {
        "id": "BxCiK5NIOccn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create test questions based on PDF content\n",
        "test_questions = [\n",
        "    \"What does a zebra look like?\",\n",
        "    \"What's the difference between a lake and a pond?\",\n",
        "    \"2 + 2 = ?\",\n",
        "]"
      ],
      "metadata": {
        "id": "MsIdhnW-Oabi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test each question\n",
        "print(\"Testing original model responses:\")\n",
        "print(\"-\" * 50)\n",
        "for question in test_questions:\n",
        "    print(f\"\\nQ: {question}\")\n",
        "    result = pipe_original(question, max_length=100)\n",
        "    print(f\"A: {result[0]['generated_text']}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "n9KM9YoYOj7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test each question\n",
        "print(\"Testing fine-tuned model responses:\")\n",
        "print(\"-\" * 50)\n",
        "for question in test_questions:\n",
        "    print(f\"\\nQ: {question}\")\n",
        "    result = pipe_finetune(question, max_length=100)\n",
        "    print(f\"A: {result[0]['generated_text']}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "oqpTcsOqOmPo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}