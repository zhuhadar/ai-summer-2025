{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPPz1TwlZMJp4VXdkhysf9A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanderbilt-data-science/ai-summer-2025/blob/main/RL_mathReasoning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "GRPO with TRL *for Reasoning*\n",
        "===============================\n",
        "\n",
        "This notebook shows how to train an autoregressive LLM to **reason through grade‚Äëschool math\n",
        "word‚Äëproblems** with the **GRPOTrainer** in ü§ó‚ÄØ[TRL Library]((https://huggingface.co/docs/trl/index).  \n",
        "\n",
        "It roughly follows the structure of our sentiment-aligning movie-hater, but we'll use a more real-world reasoning-based dataset instead!\n",
        "\n",
        "Key differences from sentiment fine-tuning\n",
        "--------------------------------------\n",
        "* **Reward signal** ‚Äì binary correctness (exact numerical answer) rather than a\n",
        "  continuous sentiment score.‚ÄØThe reward is +1 if the model‚Äôs answer matches the\n",
        "  ground‚Äëtruth answer, ‚àí1 otherwise.  This makes the optimisation landscape much\n",
        "  sharper.\n",
        "* **Prompt diversity** ‚Äì we use the full *GSM8K* dataset (~7‚ÄØk unique problems),\n",
        "  so no synthetic template expansion is required.\n",
        "* **Parsing** ‚Äì we must extract the model‚Äôs final numeric answer from its\n",
        "  chain‚Äëof‚Äëthought output to evaluate reward.\n",
        "* **Œ≤ (KL) weight** ‚Äì reasoning requires larger policy moves; we therefore set\n",
        "  `beta = 0.02`, lower than TRL‚Äôs default (0.04).\n",
        "\n",
        "---\n",
        "\n",
        "#### References and Further Reading\n",
        "* GRPO reasoning tutorial ‚Äì HuggingFace cookbook ([Tutorial](https://colab.research.google.com/github/huggingface/cookbook/blob/main/notebooks/en/fine_tuning_llm_grpo_trl.ipynb))\n",
        "* GSM8K GRPO demo repo ([GitHub](https://github.com/Yeok-c/grpo-gsm8k-demo))\n",
        "* Bite: How DeepSeekR1 was Trained ([Blog](https://www.philschmid.de/deepseek-r1))\n",
        "* Abbie's RL Tutorial (PPO-focused) ([Tutorial](https://apetulante.github.io/posts/RL-for-LLMs/RL_for_LLMs.html))"
      ],
      "metadata": {
        "id": "ZQQSuOUc3ttr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup\n",
        "\n",
        "A lot of our setup will be the same as our movie hater notebook. We proceed by:\n",
        "1. Installing dependencies\n",
        "2. Importing packages\n",
        "3. Loading our model and tokenizer\n"
      ],
      "metadata": {
        "id": "k_OENL_j3_Fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install --upgrade \"trl==0.15.2\" \"transformers>=4.40.1\" accelerate datasets math_verify --progress-bar off"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YgaD0NL4Cmv",
        "outputId": "1f607235-3a3f-4c73-d110-a74d50bb9ed3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "from trl import GRPOTrainer, GRPOConfig\n",
        "import re, random\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, set_seed\n",
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "set_seed(42)\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fODx6WA-4ET-",
        "outputId": "2d520ed3-7374-463f-8389-5dd11a6fc6ac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"vicgalle/gpt2-open-instruct-v1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# TRL requires a pad_token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device) #can rerun to reset the model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397,
          "referenced_widgets": [
            "62fd803d7ec9466f9dab97615213b902",
            "9b5c50f88ae84a089f6ca1a9bb301364",
            "67726cd5a6444328b8e6b27df1396cc5",
            "912a4bc6039a4c74870108d00ab33645",
            "bc57ec3682b54dbc9139aed249146063",
            "1c4aea38315d4671b0b2f672d249c722",
            "c9ee4b77bab742c2beaedf17df8e541e",
            "dc0973a7ad4242919837e1080002f7a8",
            "5eeb115e0646455d8c75c5de82e9bf89",
            "b4574e9c2965410ab030b8fad46205e6",
            "d41e4216ab754ef79e461c115c7f9044",
            "cebdfc134cb44f12bd4d1f50700f6c19",
            "c038697270cb4e469032aa1d3709c7ee",
            "a7ea8eeb787740cda9d3ddc930148c93",
            "2247b3501acd4148b88412d6e08931d6",
            "2883e0c4fe34481294553708ea892ecb",
            "46509921b083482496fbf4fa0c0834c7",
            "cf654e3a79c645929878100b0d52b53c",
            "babbcd92cd994e0999cc3acb9023ac3e",
            "995d88eff598403a90752be1764e5df5",
            "d2f454aa7c984236b8d64fc2b4e6a395",
            "7a32b1de1ace4e2bbfecb1426ccb52df",
            "ff52137022dc4169ac74f866dcf47a60",
            "4f26752338da42718167f7847bf73741",
            "4a82e33841ae424bb120306c439d7e95",
            "d6aa84bc03c94c8ab2c66037edeb799e",
            "46335a823dd64a8c8f9c5f821c3c863f",
            "cb4793a0d4d14a428af5d1864b798458",
            "5beccf3481a14cb6a53a9e443524dbe8",
            "40c9d010a37c4661b46d7d6edc1a024a",
            "91211a98d357478b98e070816c19b918",
            "a9ae1ecfe8ee4286a26189ae3b8af542",
            "545d9dabf98548d3b45c6deb4f2dd672",
            "62728c7a96e34f5da232b72a6d0a5688",
            "6f0d210554c84bb7b52ca6fbf1c090db",
            "771fc1f4a32a49c0bc9594fb102b9781",
            "dd06ff41cbec4efa8d26e6c712fd86f5",
            "fecf626f063a41b7aa0124ed94e7e7b0",
            "92fae5918fe74b289853e2cf534c9e9e",
            "95161a4603c7497eaded2ed028177568",
            "d96d5af737a5495c9d26ed61009ed59b",
            "5c94742ce36e4232861376da200649df",
            "7650e9c75d864af0a904b9352c02be3f",
            "d7dcb1cf796e479eb0d4e4a9b84ee8fe",
            "12fd6c7a32004ba1a1a34d2d9e359056",
            "b7401b0131bd42f4b9c505225a682e4c",
            "a430b08cb646449f9758f693f8e8c4f3",
            "ad0df21c8f154a09ae5b1dc1142f1334",
            "72c0414765884c1fa1d808e8b9964a39",
            "84bdc13ff0714b469caace69a0f8fa24",
            "3cdcc43a3c0f4c11beee16660e738c12",
            "0db9e7c514134176b165632460e896e9",
            "1cd730437d6143949de872d5c0a880db",
            "e6398c22a8ce4e7fa88bd4b515301369",
            "ce3a7c1c98794f51a43c0cf9d3ce51d6",
            "4ad734c765b040958a1d59e2b8c0eb6d",
            "e9d15b24b1bd441393c04f8d501fe74a",
            "bb76c773cd3e422c82387063af154672",
            "b4f61e7ca20b42c5ba69b4fcd23fe77a",
            "705bf22fc92049c280569848fef032a5",
            "bcff4e1eda36419d964183db180fc29f",
            "c95ddc1da59e439e825c3d2ec2eddcec",
            "87c668105fa347a98b40d159b7b8b648",
            "d9e35d6c7a3745a1a1be7bbff6ca0716",
            "febb5daca29f4bdd95dda3f35fa96898",
            "2a60af7871634959a7b6009b1b0b0413",
            "8143dc153900489d8a542e4a4fa1b0e8",
            "531938ac4d0842059d10cf7aaa51c935",
            "51738caeb23e402f994ace6e88d4865a",
            "4e15f927eb23444dbe515411c208987e",
            "77c19e0201d544f9b4f3114901fb7e22",
            "81c274ee2082486a81478455371fae2a",
            "f0eaf94023de4995a956fadbca53c1ad",
            "1d684211c5384f3083e27a61cd09be31",
            "f0c727bc28dc4581890f920e28ff01f1",
            "687aa3fd6e4a476fa286b844fe14a2eb",
            "b9c4e808301d4d6aab2072d8ed5cfb0e",
            "e428bc4db0e94483912f4724ea9f9efa",
            "fa21aa208f694a28a1805c354c97fe90",
            "6f5bd7b8915849dc9dc25c6381c18052",
            "db11c28429594ac2a38ad1b36c8f2195",
            "0f9e4f76a2f24f5d87132aceffcb9601",
            "456e5d766bc54acd8cb6cb747f08010d",
            "52aff109765d4c33a5616c26dd1ca2ec",
            "da03769238bf4ae7802819e1687056c2",
            "fd3fbe3f0b014365a3487f70dc00fe9a",
            "1225d1697a944fb9b92a7664e51b81aa",
            "d5751875ff4b49b6b5706fe1e56e0c67"
          ]
        },
        "id": "vk72bX-tmppl",
        "outputId": "1a2481f6-8aae-41ef-d9ae-abaa476fcf5a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/255 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62fd803d7ec9466f9dab97615213b902"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cebdfc134cb44f12bd4d1f50700f6c19"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff52137022dc4169ac74f866dcf47a60"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62728c7a96e34f5da232b72a6d0a5688"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/80.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "12fd6c7a32004ba1a1a34d2d9e359056"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/230 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ad734c765b040958a1d59e2b8c0eb6d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/908 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8143dc153900489d8a542e4a4fa1b0e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/510M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e428bc4db0e94483912f4724ea9f9efa"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model_obj, prompt, max_new=40):\n",
        "    inp = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    out = model_obj.generate(**inp, max_new_tokens=max_new)\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "pDWdqI4zXQWF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Load GSM8K Dataset\n",
        "\n",
        "GSM8K is a benchmark dataset of grade-school math problems written in natural language. It was designed to evaluate arithmetic and reasoning capabilities in language models, with problems requiring step-by-step logic to reach a final answer.\n",
        "\n",
        "### Why this dataset requires reasoning\n",
        "\n",
        "Unlike movie reviews (where the sentiment model just learned to be critical),\n",
        "each GSM8K prompt presents a math word problem that requires **logical steps**\n",
        "to arrive at the final answer.\n",
        "\n",
        "Examples:\n",
        "- \"Tom has 3 apples, buys 2 more...\" ‚Üí requires addition\n",
        "- \"A train leaves at 3:30 and arrives at 5:10...\" ‚Üí time subtraction\n",
        "\n",
        "The model must not only read and understand the problem but execute the\n",
        "appropriate arithmetic steps before writing `Answer: <number>` at the end.\n",
        "\n",
        "This makes the task more sensitive to coherence, correctness, and following\n",
        "multi-step structure ‚Äî a stronger test of reasoning.\n",
        "\n",
        "We keep 5000 problems for training to keep GPU time semi- reasonable. It will still take a long time - lower to 1000 (or even fewer!) to get through the notebook more quickly if you just want to see it run!"
      ],
      "metadata": {
        "id": "WxDjJV8k4LLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_ds = load_dataset(\"gsm8k\", \"main\", split=\"train[:1000]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "c1a864ebbbd145718fcd457b483ab118",
            "d45c09967ebe4ee8bfe9293a12719aa8",
            "bc973fcca9ac4f1a9cdd67f77a6d8d46",
            "dbd32e1e132748cb8b6cfdd1feac5587",
            "25fad015784c4e9284429264edb3d4e8",
            "49a1fb4926a44a42b501e4c74bec9aea",
            "3a02d4786f7541bd8a126465e7965240",
            "aa460a4ad4644478b3ae3dc61762e52f",
            "289cf3e5d931489293619f6cce6295ce",
            "cd90840d760b4343a591e024cd231528",
            "89b6907b3340400ab80c13458e200320",
            "a0e268d4f24b42a08d23875350e83a4e",
            "4f58d028d31e4213b184ce79d3a2d256",
            "bf0c1bc1ab05407eb9b6eed016abcf38",
            "60224abce19344e6be21682a528f6413",
            "951ef96819214273ac75bcca5bfcea48",
            "ba9e0294e6574bba9893791f25010a40",
            "88c3b545491e4729b85b1e82dd61f484",
            "b0d85f92d79c44f6881a041d09fd46e0",
            "37873b9a0ea84229932d65884208cb4d",
            "a377172a02d34e7f90026aa4fda5f243",
            "45bac44f270e4959b620856976c4c14a",
            "46ec0aa38dda4533ae05fb1144a7b865",
            "1d0b79049bc84033ac56937cd41e11d2",
            "222d344211ca4c339a22465f1bc85768",
            "30ae46cba367487e942e177c23d2be99",
            "45370c16b7f042a982b10e76989347e1",
            "f345329331bc434682c911d89f18d3c1",
            "318e8a43f4174d7e8b21a2eefd92e63e",
            "b094aa9aea27422d88e0904f74bb386a",
            "21902001f0e54cb786d97073224d5b31",
            "5e186d802ce241e1bd510e0ad2b010ab",
            "ef0170f3bb3348b6b9fd5e200f138e60",
            "078841c6a27e407197d60db188e48298",
            "6f809a79a7c34b539075451039514bd4",
            "bf4a0992e5084d4c85068772e79e37be",
            "3e91c4ae8b884f96849f9dc5fefb491f",
            "8cf538b4676247619d3cf2c5edbc02ba",
            "2f5c8880f6764e619c10a5cca3263ad8",
            "a542cbcc7ee648a2a4d8bc3395f4fde2",
            "e41881413da5437f87907a80e26c842c",
            "ddd125c9f82e44ca8a32e1cefae31ccf",
            "ce22be92d29148acbe2baf83f24b0edd",
            "d4540efca62c4ccbb2d1a3a9bd14d6e9",
            "053f54e8fd2349db95c84e5359269383",
            "29fc58a9e2c54b839d2501e2a37cd8f8",
            "4354747596bc41ae9498c1bdc5e70232",
            "90affec8e1364850b67c15aac1665436",
            "9088a00f9de24085999302459d7f123b",
            "d9b0e46b58414d8c8e61f45f8b94c518",
            "d62bc413fc37484096d64c286cc9417b",
            "56d60b11612c4c43aaf77da069575d97",
            "53e4c0f8172a4055aad22ae18c23e441",
            "a4ac44a00f674e9cbd8e4e5ea0ee0b2e",
            "f2fceaae90e6419c9f5d2be1b713d0d9"
          ]
        },
        "id": "pk0a1EMh4NyR",
        "outputId": "f7ffe792-7155-41f2-cdaa-c60442c21789"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/7.94k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1a864ebbbd145718fcd457b483ab118"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0e268d4f24b42a08d23875350e83a4e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46ec0aa38dda4533ae05fb1144a7b865"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "078841c6a27e407197d60db188e48298"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "053f54e8fd2349db95c84e5359269383"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can inspect one of these dataset examples:"
      ],
      "metadata": {
        "id": "rhoW5DjYvHL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Question: \" + raw_ds[0][\"question\"])\n",
        "print(\"Answer: \" + raw_ds[0][\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fe_6JwX1vORj",
        "outputId": "0aea72a2-4be4-4a97-a629-ecc7d2abe456"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
            "Answer: Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
            "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
            "#### 72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And see how our model responds before training:"
      ],
      "metadata": {
        "id": "-vIN2TeCYSil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate(model, raw_ds[0][\"question\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ta7UknvnYXbg",
        "outputId": "29797e19-4053-42de-8565-53bd746a3d13"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
            "\n",
            "April: 2,000\n",
            "May: 2,000\n",
            "\n",
            "April: 2,000\n",
            "\n",
            "April: 2,000\n",
            "\n",
            "April: 2,000\n",
            "\n",
            "April: 2,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Build Prompts\n",
        "Each prompt asks the model to solve the problem *step‚Äëby‚Äëstep* so it can show\n",
        "its reasoning.\n",
        "\n",
        "Then, we insert from our dataset explicitly the answer that we extract from the dataset. We'll format our data to have the reasoning process between think tags and the final answer separate, i.e.\n",
        "\n",
        "Prompt format:\n",
        "```\n",
        "<problem>\n",
        "Jack has 3 apples. ‚Ä¶\n",
        "</problem>\n",
        "<think>\n",
        "3 + 4 = 7  \n",
        "7 ‚Äì 2 = 5  \n",
        "</think>\n",
        "<answer>5</answer>\n",
        "```"
      ],
      "metadata": {
        "id": "41yAXjZF4QMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = []\n",
        "answers = []\n",
        "\n",
        "for ex in raw_ds:\n",
        "    q   = ex[\"question\"]\n",
        "    txt = ex[\"answer\"]\n",
        "\n",
        "    # 2. Extract final answer (after \"#### \")\n",
        "    m_ans = re.search(r\"####\\s*([-+]?\\d+)\", txt)\n",
        "    if not m_ans:\n",
        "        continue\n",
        "    ans = m_ans.group(1)\n",
        "\n",
        "    # 3. Extract chain-of-thought (everything before the #### marker)\n",
        "    cot = txt.split(\"####\")[0].strip()\n",
        "\n",
        "    # 4. Build formatted prompt\n",
        "    prompt = (\n",
        "        \"<problem>\\n\" + q + \"\\n</problem>\\n\\n\"\n",
        "        \"<think>\\n\" + cot + \"\\n</think>\\n\\n\"\n",
        "        f\"<answer>{ans}</answer>\"\n",
        "    )\n",
        "\n",
        "    prompts.append(prompt)\n",
        "    answers.append(int(ans))\n",
        "\n",
        "\n",
        "dataset = Dataset.from_dict({\"prompt\": prompts, \"answer\": answers})"
      ],
      "metadata": {
        "id": "nq3ybYH24VxD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAvn-61BfcNC",
        "outputId": "a6b28a70-e8ac-4a20-f3bb-420a21c7bd08"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'prompt': '<problem>\\nNatalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\\n</problem>\\n\\n<think>\\nNatalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n</think>\\n\\n<answer>72</answer>',\n",
              " 'answer': 72}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Reward Function: Answer Match\n",
        "\n",
        "Now, let's build a reward function that tries to make sure we get not just a correct answer, but also some good reasoning!\n",
        "\n",
        "We'll still keep it super simple. We give rewards:\n",
        "\n",
        "- **+0.4** if the `<answer>` matches the ground truth,\n",
        "\n",
        "- **+0.2** for each correctly verified line inside the `<think>` block\n",
        "\n",
        "This way, we ensure we get the correct final answer, but also check that the model's thinking steps are arthimetically correct."
      ],
      "metadata": {
        "id": "k-rT-hjk4e6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math_verify import parse, verify\n",
        "from math_verify.parser import ExprExtractionConfig\n",
        "\n",
        "# Function to check if a given line in the thinking process is correct\n",
        "def verify_step(line: str) -> bool:\n",
        "    \"\"\"\n",
        "    Returns True if `line` is of the form \"A op B = C\" and the arithmetic checks out,\n",
        "    using Math-Verify's parser and verifier.\n",
        "    \"\"\"\n",
        "    if \"=\" not in line:\n",
        "        return False\n",
        "    lhs, rhs = map(str.strip, line.split(\"=\", 1))\n",
        "\n",
        "    # parse both sides as plain expressions (no LaTeX)\n",
        "    left_expr  = parse(lhs, extraction_config=[ExprExtractionConfig()])\n",
        "    right_expr = parse(rhs, extraction_config=[ExprExtractionConfig()])\n",
        "\n",
        "    # verify expects (gold, prediction), so ensure order:\n",
        "    # gold = RHS, prediction = LHS\n",
        "    return verify(right_expr, left_expr)\n",
        "\n",
        "# The reward function that will look at the thinking process AND final answer\n",
        "def reward_fn(prompts, completions, answer):\n",
        "    rewards = []\n",
        "    for out, gt in zip(completions, answer):\n",
        "        # extract think-block and answer-block\n",
        "        think = re.search(r\"<think>(.*?)</think>\", out, re.S)\n",
        "        ans   = re.search(r\"<answer>(\\d+)</answer>\", out)\n",
        "        score = 0.0\n",
        "        if think:\n",
        "            # verify each line in think: simple arithmetic check\n",
        "            for line in think.group(1).splitlines():\n",
        "                if verify_step(line):  # your step-checking logic\n",
        "                    score += 0.2\n",
        "        if ans and int(ans.group(1)) == gt:\n",
        "            score += 0.4\n",
        "        rewards.append(score)\n",
        "    return rewards"
      ],
      "metadata": {
        "id": "_nK6Q9zW4dkM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test this reward quickly to ensure that we get what we expect"
      ],
      "metadata": {
        "id": "xOOZxxJukDRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pick example i\n",
        "i = 0\n",
        "prompt_i     = prompts[i]\n",
        "gold_output  = prompts[i]   # grab from what we formatted from the data earlier\n",
        "gold_answer  = answers[i]\n",
        "\n",
        "# compute reward\n",
        "reward = reward_fn(\n",
        "    prompts     = [prompt_i],\n",
        "    completions = [gold_output],\n",
        "    answer     = [gold_answer]\n",
        ")[0]\n",
        "\n",
        "print(\"Training example reward:\", reward)\n",
        "\n",
        "# now, try to put something blatantly wrong as answer\n",
        "reward = reward_fn(\n",
        "    prompts     = [prompt_i],\n",
        "    completions = [gold_output], # the thinking steps will still be correct\n",
        "    answer     = [gold_answer + 10] # BUT we put an answer here that we know is wrong!\n",
        ")[0]\n",
        "\n",
        "print(\"Wrong example reward:\", reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FEHIoQ9ji6B",
        "outputId": "3164f336-a104-4136-8afc-f2cc5441f8a8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
            "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
            "Training example reward: 0.8\n",
            "\n",
            "Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
            "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
            "Wrong example reward: 0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "While our reward function effectively incentivizes correctness (even along the way) it does not necessarily mean we'll get *optimal* intermediate reasoning steps. But, we are checking that the reasoning wasn't totally off the rails, at least somewhat!\n",
        "\n",
        "To train models that value coherent, concise, AND accurate reasoning processes, alternative reward modeling techniques can be employed that incentivize good reasoning paths. Indeed, it's common for a reward function to give rewards for multiple elements of \"goodness\" in an answer simultaneously!\n",
        "\n",
        "*Also note! Our checker for line-by-line accuracy is VERY simple here, and may fail on some examples in this dataset which, for instance, use varaibles in the math expressions. Bulding a robust reward function is a crucial step of RL training!*"
      ],
      "metadata": {
        "id": "cJUs_jk1xApp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. GRPO Configuration\n",
        "\n",
        "We set up GRPO largely the same as in our sentiment notebook. Note one change is to make sure that the max completion length is longer, as we want to allow longer response generations when reasoning is required."
      ],
      "metadata": {
        "id": "ie3AySeP4lPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = GRPOConfig(\n",
        "    beta=0.02,  # Controls the strength of the KL divergence penalty; higher values keep the model closer to the reference policy.\n",
        "    learning_rate=5e-6,  # Determines the step size at each iteration while moving toward a minimum of the loss function.\n",
        "    num_generations=4,  # Number of completions generated per prompt; facilitates diverse outputs for better policy optimization.\n",
        "    per_device_train_batch_size=64,  # Number of samples processed per device in one forward/backward pass; must be divisible by num_generations.\n",
        "    gradient_accumulation_steps=4,  # Number of steps to accumulate gradients before updating model weights; helps simulate larger batch sizes.\n",
        "    logging_steps=10,  # Frequency (in steps) at which training logs are recorded.\n",
        "    max_prompt_length=64,  # Maximum number of tokens in the input prompt; inputs longer than this will be truncated.\n",
        "    max_completion_length=128,  # Maximum number of tokens the model can generate in response to a prompt.\n",
        ")\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,  # Our model (loaded above)\n",
        "    args=cfg,  # Training configuration (defined above)\n",
        "    train_dataset=dataset,\n",
        "    reward_funcs=[reward_fn],  # List of reward functions to evaluate generated outputs. Note can be more than one!\n",
        "    processing_class=tokenizer,  # Tokenizer corresponding to the model\n",
        ")\n"
      ],
      "metadata": {
        "id": "i9vQm3H64p7U"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Train\n",
        "This will take a long time! Even on an A100, it may take several hours for this step to run. We've chosen a fairly large set of examples here, and generation for reasoning takes a bit longer as we've allowed the maximum response length to be longer.\n",
        "\n",
        "If you re-run this code, this section will ask for an API key for weights and biases."
      ],
      "metadata": {
        "id": "ux_1kEVS4wno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0z6V8PdhedTi",
        "outputId": "641b4531-df25-419c-87fc-65c15472e3a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabigail-petulante\u001b[0m (\u001b[33mabigail-petulante-vanderbilt-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250516_152136-eznvthba</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/abigail-petulante-vanderbilt-university/huggingface/runs/eznvthba' target=\"_blank\">trainer_output</a></strong> to <a href='https://wandb.ai/abigail-petulante-vanderbilt-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/abigail-petulante-vanderbilt-university/huggingface' target=\"_blank\">https://wandb.ai/abigail-petulante-vanderbilt-university/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/abigail-petulante-vanderbilt-university/huggingface/runs/eznvthba' target=\"_blank\">https://wandb.ai/abigail-petulante-vanderbilt-university/huggingface/runs/eznvthba</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`generation_config` default values have been modified to match model-specific defaults: {'use_cache': False, 'bos_token_id': 50256, 'eos_token_id': 50256}. If this is not desired, please set these values explicitly.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The first snake is 24 inches because there are 12 inches in a foot.\n",
            "The snakes are 24+16+10= <<24+16+10=50>>50 inches long.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='27' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [27/45 29:53 < 21:31, 0.01 it/s, Epoch 1.63/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>108143526.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.368000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "<think>\n",
            "<think>\n",
            "<think>\n",
            "<think>\n",
            "<think>\n",
            "<think>\n",
            "<think>\n",
            "\n",
            "<h1>As per the rule of Euclidean mechanics, ‚Äú[1, 2, 3, 6, 6]** is equal to 180.‚Äù</h1>\n",
            "120\n",
            "0.75\n",
            "27\n",
            "\n",
            "At first the children had 10*7=<<10*7=70>>70 books. With their teacher, they have 70+8=<<70+8=78>>78 books.\n",
            "6\n",
            "I know, but I'll get my mind blown. I'll just take a few moments to think. Please, try this out with a few more minutes.\n",
            "200 x 3 = <<50*3=150>>150 kg of fish.\n",
            "Therefore, he sold 150 x 3 = <<50*3=150>>150 kg of fish.\n",
            "10\n",
            "\n",
            "80\n",
            "\n",
            "\n",
            "75\n",
            "2240\n",
            "\n",
            "Female:36(.50)=17 cows\n",
            "14\n",
            "\n",
            "You said that you don't want to waste money on something when it can be used to get something you want. For example, you will never need $200 to get a new pair of shoes, or to upgrade your car if you don't have the money.\n",
            "\n",
            "\n",
            "<i>The novels left behind by this writer</i>\n",
            "<ii>10</ii>\n",
            "<iii>9</iii>\n",
            "<iv>10</iv>\n",
            "\n",
            "Laura expects 150 - 3 = <<150-3=65>>65 Guests.\n",
            "Laura expects 200 * 0.3 = <<200-3=65>>65 Guests.\n",
            "\n",
            "35</span>\n",
            "\n",
            "He got 20*3=<<21*3=60>>60 seeds\n",
            "That means he plants 20*.6=<<20*3=24>>24 trees\n",
            "I was thinking about whether or not to accept the fact that the animals could have eaten a lot of animals. I want to know if we can do more than 50% of the work with the animals and if that will outweigh the cost.\n",
            "\n",
            "Shelly bought 12/2=<<12/2=4>>4 pretzels\n",
            "Angie bought 6*3=<<6*3=18>>18 pretzels\n",
            "c\n",
            "\n",
            "<can>\n",
            "</can>\n",
            "320\n",
            "91\n",
            "\n",
            "221\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Test Performance"
      ],
      "metadata": {
        "id": "rpS8jRE840PH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsiVNpyejTsj"
      },
      "outputs": [],
      "source": [
        "test_prompt = raw_ds[0][\"question\"]\n",
        "before = generate(model, test_prompt)\n",
        "after  = generate(trainer.model, test_prompt)\n",
        "\n",
        "print(\"üîµ BEFORE\\n\", before, \"\\n\")\n",
        "print(\"üü¢ AFTER\\n\", after, \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(*NOTE:* I cut the training short! Answers would likely be at least slightly better if training completed. But it takes a long time!)\n",
        "\n",
        "So, this behavior is definitely more reasoning-like. But our model didn't exactly arrive at the right answer through clear and logical thinking.\n",
        "\n",
        "For one, we still didn't use much data (5000 examples is pretty small).\n",
        "\n",
        "Also, while our dataset did provide walkthrough answers which we scored the steps of *along with* final answer, true, good reasoning is more than just mathematical correctness of intermediate steps."
      ],
      "metadata": {
        "id": "mN0xQMjNa12w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practice, these models are trained on *many, many more examples* and with *much more complex reward functions*.\n",
        "\n",
        "For example, DeepSeek-R1 goes well beyond a simple ‚Äúcorrect/incorrect‚Äù final answer reward. Instead, it's training pipeline combines:\n",
        "\n",
        "- **Answer Reward** ‚Äì a binary signal for a fully correct final answer, evaluated by a programmatic verifier\n",
        "\n",
        "- **Format Reward** ‚Äì a small bonus when the model follows prescribed structure (e.g. `<think>‚Ä¶</think>` and `<answer>‚Ä¶</answer>` tags)\n",
        "\n",
        "- **Constraint Rewards** ‚Äì checks that each input number is used exactly once and computations adhere to the problem‚Äôs rules\n",
        "\n",
        "- **Language/Fluency Rewards** ‚Äì soft penalties for disfluent or non-canonical phrasing, to encourage readable reasoning\n",
        "\n",
        "Optimizing not just for strong ability to get the right answer, but also to get there in efficient and coherent ways!"
      ],
      "metadata": {
        "id": "gbIqV41Lb-Oh"
      }
    }
  ]
}
